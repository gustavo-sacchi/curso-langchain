
# Utilizando APIs Gratuitas

Se você deseja fazer o curso de LangChain, mas não quer pagar a API da Open AI existem algumas alternativas:

### API Groq

Site: https://console.groq.com/keys

A Groq é a empresa que oferece um serviço completo mesmo que limitado aos [modelos open source](https://console.groq.com/docs/models) que eles hospedam na infraestrutura deles. Neste link você consegue ver os limites para contas gratuitas: https://console.groq.com/docs/rate-limits

- Precisa se cadastrar para obter a chave API

### API Aistudio da Google

Site: https://aistudio.google.com/welcome

A Google oferece uma chave API para desenvolvedor com taxa limitada, porém sem custo para que você possa realizar testes utilizando os modelos de IA da Google.

- Precisa se cadastrar para obter a chave API
### API Anthropic

Site: https://www.anthropic.com/api

Até o momento da gravação deste tutorial, a Anthropic oferece 5 dólares de crédito na API para quem fazer o cadastro completo incluindo o numero de celular.

- Precisa se cadastrar para obter a chave API

### API Together.ai

Site: https://www.together.ai/

Até o momento da gravação deste tutorial, a Together oferece 5 dólares para quem não tem conta para testar os modelos open source que eles disponibilizam na plataforma isso se for sua primeira vez criando conta.

- Precisa se cadastrar para obter a chave API

### HuggingFace

Site: https://huggingface.co/

Se você tem cadastro na huggingface você pode criar uma chave API para fazer o teste à alguns modelos que eles hospedam.

- Precisa se cadastrar para obter a chave API

# Utilizando Modelos Locais via Ollama

Se você tem preferencia pro privacidade e possui uma infraestrutura robusta como por exemplo placa de video (GPU) em seu computador, você pode trabalhar com modelos Open Source quantizados localmente utilizando o Ollama. O Ollama oferece uma API para que possamos trabalhar com mais facilidade com as chamadas aos modelos que estão rodando localmente.

O ponto negativo é que para rodar modelos bons que tendem a ser grandes, você precisa ter uma infraestrutura muito robusta para conseguir descarregar o modelo em seu computador e rodar as inferências. 

Nesse caso convido a realizar o download do serviço gratuito em https://ollama.com/ onde você pode também encontrar os modelos disponíveis para baixar.

# Como adaptar os tutoriais para rodar os modelos tanto locais quanto de APIs Gratuitas?


1) Se cadastrar na plataforma que oferece o serviço gratuito de acesso aos modelos de inferência. 
2) Obter a chave API e verificar na documentação qual o nome correspondente que deve ser adotado no arquivo `.env`. Na documentação da LangChain você consegue encontrar as integrações possíveis com o framework e como utilizar o ChatModel correspondente. Verifique em: https://python.langchain.com/docs/integrations/chat/
3) Instalar as dependências. Por exemplo::

```python
# Google
pip install langchain-google-genai

# Groq
pip install langchain-groq
```

4) Por exemplo se você for usar a API da Groq, no arquivo `.env` você precisará preencher: `GROQ_API_KEY=ADD_KEY_AQUI`, para o Aistudio da Google, `GOOGLE_API_KEY=ADD_KEY_AQUI`
5) Em seguida, quando utilizarmos no tutorial algum `ChatModel` da OpenAI, você precisará trocar para o `ChatModel` correspondente à API que você está utilizando, por exemplo:

```python
from langchain_openai import ChatOpenAI
model = ChatOpenAI(model="gpt-4o", temperature=0.2)

# Trocar por:
from langchain_google_genai import ChatGoogleGenerativeAI
model = ChatGoogleGenerativeAI(model="gemini-1.5-pro", temperature=0.2)

# ou
from langchain_groq import ChatGroq
model = ChatGroq(model="llama-3.1-8b-instant", temperature=0.2)

```

Na documentação do LangChain você pode encontrar como fazer essa substituição. E lembre-se que o carregamento das chaves APIs nós estamos fazendo usando o `dotenv` quando colocamos em nosso arquivo o trecho:

```python
from dotenv import load_dotenv  
load_dotenv()
```

Por isso, não precisamos enviar explicitamente dentro do `ChatModel` a chave API

---

Se você for utilizar o Ollama normalmente o serviço roda em `localhost` e a porta é `11434`, então, você precisará fazer a seguinte alteração:

```python
ChatOllama(  
     base_url="http://localhost:11434",  
     model = "phi4",  # nome do modelo que deseja tem em sua máquina
     temperature = 0.3,  
     num_predict = 1000, # numero máximo de tokens
 )
```

Nesse caso não tem chave API.