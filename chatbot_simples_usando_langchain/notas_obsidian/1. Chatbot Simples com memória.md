[Importante](https://www.analyticsvidhya.com/blog/2024/06/langchain-expression-language/)
# Passos

- Primeiramente criar o ambiente virtual de desenvolvimento.
- Instalar o LangChain e dependências

```python
pip install langchain
pip install langchain-openai
pip install langchain-groq
pip install langchain-community SQLAlchemy

```

- Criar uma chave API em um provedor de LLM. Vamos usar o GPT da Open IA: [OpenAI](https://platform.openai.com/api-keys) | [Groq](https://console.groq.com/keys)
- Importar as classes necessárias para desenvolvimento da Chain.

```python
# from langchain_groq import ChatGroq  
from langchain_openai import ChatOpenAI  
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder  
from langchain_core.output_parsers import StrOutputParser  
from langchain_community.chat_message_histories import SQLChatMessageHistory  
from langchain_core.runnables import RunnablePassthrough, RunnableWithMessageHistory
```

- Estruturar primeiramente uma cadeia conversacional simples
	- Crie uma estrutura de Chat Prompt, componente que gerencia as mensagens de sistema, usuário e ia.
	- Crie um modelo de LLM, componente responsável por criar a comunicação entre usuário e modelo de IA.
	- Crie um analisador de saída, componente responsável por formatar a saída do modelo de geração de texto.
- Criar e conectar um banco de dados para manter as conversas por sessão de usuário
- Com base em IDs de sessão de usuário específicos, encapsule a memória na cadeia de chatbot.
- Crie um sistema que gerencie a janelas de mensagens que serão enviadas para o modelo juntamente com a entrada do usuário.
- Por fim, execute a cadeia usando "invoke" passando a mensagem do usuário como parâmetro de entrada.
- Imprima a resposta.

Código completo:

```python
# criar sistema de gestão do prompt: sistema + usuário + histórico + ia  
prompt = ChatPromptTemplate.from_messages([  
    ("system", "Atue como um assistente de IA útil"),  
    MessagesPlaceholder(variable_name="history"),  
    ("human", "{human_input}")])  
  
# persistir todas as conversas baseadas em sessão do usuário em um banco de dados SQL  
def get_session_history_db(session_id):  
    return SQLChatMessageHistory(session_id, connection="sqlite:///memory.db")  
  
# crie uma função de janela de buffer de memória para retornar as últimas K conversas  
def memory_window(messages, k=10):  
    return messages[-(k+1):]  
  
# crie uma cadeia LLM simples que usa apenas as últimas K conversas  
chatgpt = ChatOpenAI(model_name="gpt-4o", temperature=0, openai_api_key="API_AQUI")  
llm_chain = (RunnablePassthrough.assign(history=lambda x: memory_window(x["history"]))  
             | prompt  
             | chatgpt  
             | StrOutputParser())  
  
# crie uma cadeia de conversação para lidar com o histórico baseado em sessão.  
conv_chain = RunnableWithMessageHistory(llm_chain, get_session_history_db, input_messages_key="human_input", history_messages_key="history")  
  
# test out the chain  
print(conv_chain.invoke(input={"human_input": "O que eu te perguntei?"}, config={'configurable': { 'session_id': "ID_USUARIO"}}))
```

Ajuste para Aceitar o groq:
```python
chatgroq = ChatGroq(model_name="llama-3.1-70b-versatile", temperature=0, groq_api_key="API_KEY_AQUI")
```