## 1.1 O que é LangChain

**LangChain** é uma estrutura (framework) desenvolvida para facilitar o desenvolvimento de aplicativos e pipelines que integram modelos de linguagem de grande porte (LLMs, como o GPT) com outras fontes de dados e componentes de software. O objetivo principal do LangChain é ajudar desenvolvedores a criar aplicações complexas que podem usar modelos de linguagem de maneira eficiente e interagir com fontes de dados externas, como bancos de dados, APIs, e outros tipos de dados dinâmicos.

A estrutura é altamente modular, ou seja, as cadeias são formadas por combinação de blocos funcionais que manipulam LLMs, memória, ferramentas externas, e fluxos de dados, para criar sistemas interativos e inteligentes.

Principio Básico: Chain (Cadeias)

As "cadeias" são um dos conceitos centrais do LangChain. Elas permitem que você encadeie várias operações que podem incluir consultas a LLMs, processamento de respostas, chamadas a APIs externas, interações com bancos de dados, etc. Em vez de simplesmente chamar um modelo de linguagem e obter uma resposta, as "chains" possibilitam que essas chamadas sejam combinadas em fluxos mais complexos.

## 1.2. LCEL

[documentação](https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel)

Uma camada de código “minimalista” para criar cadeias de componentes LangChain é possível graças à LangChain Expression Language (LCEL), que é uma abstração de algumas ideias intrigantes do Python. Ela basicamente usa o operador pipe, que é semelhante aos comandos Unix, onde podemos passar a saída da função anterior para a próxima função usando o operador pipe.

Usando LCEL, criamos nossa cadeia de forma diferente, usando operadores de pipe (|) em vez de objetos Chains.

O LCEL vem com forte suporte para:

- Desenvolvimento super rápido de correntes.
- Recursos avançados como streaming, execução assíncrona, paralela e muito mais.

Ao utilizar a linguagem LCEL precisamos conhecer os componentes mais básicos de construção de uma chain:

- LLM (*model*): componente abstrato que se conecta ao modelo gerador de texto.
- Prompt (*prompt*): template utilizado para compor a string da cadeia pré envio ao modelo de linguagem,
- Analisador de saída (*output_parser*): um analisador que define como extrair a saída da resposta do LLM e exibi-la como resposta final.

Logo,
```python
lcel_chain = prompt | model | output_parser
```

Além disso, cada objeto da chain implementam o protocolo executável (Runnable), ou seja, todo executável carrega a interface padrão síncrona e assíncrona:

- `stream` / `astream`: transmitir de volta pedaços da resposta
- `invoke` / `ainvoke`: chamar a cadeia em uma entrada
- `batch` / `abatch`: chama a cadeia em uma lista de entradas


