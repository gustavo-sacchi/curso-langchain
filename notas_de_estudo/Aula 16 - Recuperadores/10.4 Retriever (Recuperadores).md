[Documentação](https://python.langchain.com/docs/how_to/#retrievers)
## 1. O que é um Retriever?

Um **Retriever** é um componente responsável por, dado um texto de consulta (query), **retornar uma lista de documentos relevantes**. O conceito de “documentos” pode variar – podem ser pedaços de texto, registros em um banco de dados, páginas da web etc. 

É importante salientar que um retriever é um `runnable`, ou seja, você pode invocá-lo. Como retorno teremos um `Document`. Cada documento é frequentemente representado por:

- **page_content**: o conteúdo do documento (string).
- **metadata**: dados adicionais (por exemplo, fonte, autor, ID, data).

A interface de retriever no LangChain é bastante simples:

1. **Entrada**: uma query (string)
2. **Saída**: lista de objetos `Document` relevantes para a query

Por trás desse processo, o retriever pode usar qualquer método de busca, desde pesquisas lexicais (BM25, por exemplo) até buscas semânticas em **bancos de dados vetoriais**. O ponto comum é que, para a aplicação final, basta chamar o retriever com a query, e ele entrega os documentos relevantes. Esse é o nível de abstração que o LangChain te entrega.

## 2. Usando um Vector Store como Retriever

Hoje em dia, um dos métodos mais populares (e eficientes) de realizar buscas relevantes em textos não-estruturados é por meio de **bancos de dados vetoriais (vector stores)**. A LangChain permite converter esses bancos em retrievers de forma direta.

### 2.1 Criação de um Vector Store Retriever

Suponha que já tenhamos um vector store pronto (por exemplo, usando Chroma, Qdrant, Milvus, Pinecone etc.). Para transformar esse vector store em um retriever, basta chamar o método `as_retriever()`:

```python
vectorstore = MyVectorStore()  # Ex.: Qdrant, Chroma, Pinecone...
retriever = vectorstore.as_retriever()
```

Com esse `retriever`, podemos chamá-lo de forma padronizada, por exemplo usando `invoke` (ou `.get_relevant_documents(query)`, dependendo da versão da LangChain):

```python
docs = retriever.invoke("Qual foi a declaração do aluno sobre IA?")
for doc in docs:
    print(doc.page_content)
```

### 2.2 Personalizando o Tipo de Busca

No caso de `vector stores` (banco de dados vetorial), a busca pode variar, ou seja, você pode pre configurar como deve ser a estratégia de similaridade parametrizando o parâmetro `search_type` e `search_kwargs`:

- **similarity_search** (padrão): busca documentos com maior similaridade vetorial.
- **maximum marginal relevance (mmr)**: procura variar mais os resultados (diversificar). Por exemplo, `search_type="mmr"`
- **limite mínimo de score** (threshold): só retorna documentos que atinjam um nível mínimo de similaridade. Por exemplo, `search_type="similarity_score_threshold"`
- **top-k**: permite definir quantos documentos retornar. Por exemplo: `search_kwargs={"k": 1}`

Por exemplo, para usar MMR:

```python
retriever = vectorstore.as_retriever(search_type="mmr")
docs = retriever.invoke("Qual foi a declaração do aluno sobre IA")
```

Ou limitar a quantidade de documentos e aplicar um limite de similaridade:

```python
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold", 
    search_kwargs={"score_threshold": 0.5, "k": 3}
)
docs = retriever.invoke("O que foi dito sobre IA?")
```

Essas configurações ajudam a controlar a qualidade e a quantidade de resultados retornados.

Leia mais em https://python.langchain.com/docs/how_to/vectorstore_retriever/

## 3. Outras Formas de Retrieval

Embora os **vector stores** sejam muito populares, a LangChain não se restringe a eles. A interface de retriever aceita diversos “back-ends” de busca:

1. **Search APIs Externas:** Por exemplo, integrar com a busca da Wikipedia ou com serviços como Amazon Kendra. Nesses casos, o retriever não armazena os documentos localmente, mas sim chama uma API que retorna resultados de busca.
    
2. **Bancos de Dados Relacionais ou de Grafos:** O retriever pode receber uma query em linguagem natural e convertê-la em SQL (text-to-SQL) ou em consultas de grafo (por exemplo, SPARQL ou Cypher), retornando os resultados relevantes.
    
3. **Lexical Search (BM25, TF-IDF):** Métodos mais clássicos baseados em palavras-chave. A LangChain também oferece integrações que permitem criar retrievers usando algoritmos de similaridade lexical. Em algumas situações, lexical search ainda pode ser útil (por exemplo, para buscar termos exatos).
    

O objetivo da LangChain é padronizar a **entrada** (string) e a **saída** (lista de documentos), independente de como a busca é feita internamente.

Leia mais em: https://python.langchain.com/docs/concepts/retrievers/

## 4. Padrões Avançados de Retrieval

À medida que as aplicações crescem em complexidade, surgem necessidades de combinar ou aperfeiçoar os métodos de busca. Alguns padrões avançados incluem:

### 4.1 Ensemble de Retrievers

É possível ter **múltiplos retrievers** diferentes (por exemplo, um usando BM25 e outro usando Vector Store) e combiná-los, ponderando os resultados de cada um.

```python
from langchain.retrievers import EnsembleRetriever

ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_store_retriever],
    weights=[0.5, 0.5]
)
docs = ensemble_retriever.invoke("Pergunta de exemplo")
```

A ideia é unificar as pontuações ou ranqueamentos de cada retriever, resultando em uma lista final de documentos que aproveita “o melhor de cada mundo”. 

Leia mais em: https://python.langchain.com/docs/concepts/retrievers/
### 4.2 MultiQueryRetriever

Um problema comum em buscas vetoriais ou semânticas é a **sensibilidade à formulação da query**. Pequenas variações na forma de perguntar podem retornar resultados diferentes. O **MultiQueryRetriever** contorna isso gerando diversas versões da query usando um LLM, executando cada versão, e depois unindo os resultados.

```python
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(temperature=0)
retriever = vectorstore.as_retriever()

multi_query_retriever = MultiQueryRetriever.from_llm(
    retriever=retriever,
    llm=llm
)

docs = multi_query_retriever.invoke("Quais são os métodos de decomposição de tarefas?")
print(len(docs))
```

O LLM produz variações da pergunta, por exemplo:

- “Como podemos dividir problemas em etapas menores?”
- “Quais técnicas de decomposição de tarefas são mais utilizadas?”
- etc.

Depois, o multi-query faz a busca para cada variação e combina os documentos relevantes, resultando em uma visão **mais abrangente** do assunto.

Leia mais em: https://python.langchain.com/docs/how_to/MultiQueryRetriever/

### 4.3 Outros

Existem outros tipos de retrievers que valem a pena consultar na fonte da documentação:

- [Self Query](https://python.langchain.com/docs/how_to/self_query/)
- [Contextual Compression](https://python.langchain.com/docs/how_to/contextual_compression/)

## 5. Conclusão

### 5.1 Por que usar Retrievers?

Os **retrievers** desempenham um papel fundamental em aplicações de **IA e NLP** que precisam obter informações relevantes de grandes volumes de dados ou misturar diferentes fontes de conhecimento. A interface padronizada da LangChain:

- Simplifica o processo de alternar ou combinar diferentes métodos de busca.
- Fornece um ponto único de integração com LLMs para criar **aplicações que buscam e respondem** a perguntas com mais precisão e contexto.

### 5.2 Recomendações Finais

- Avalie o tipo de dado e o caso de uso para escolher o método de busca.
- Se os resultados estiverem muito homogêneos ou superficiais, considere **modelos avançados** (como MMR) ou o uso de **MultiQueryRetriever**.

Com esses conceitos, você terá uma base sólida para adicionar **retrieval** (recuperação de informação) em suas aplicações, integrando com LLMs ou outras ferramentas de NLP para obter respostas relevantes e contextualizadas.