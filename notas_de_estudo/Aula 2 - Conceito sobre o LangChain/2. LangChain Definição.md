
# Modelos de linguagem e NLP

Fonte: https://newrizon.global/blog/entendendo-o-funcionamento-do-chatgpt-de-forma-simples-um-guia-para-nao-tecnicos/

Existem muitos tipos de IA ou modelos de _deep learning_. Para tarefas de Processamento de Linguagem Natural (Natural Language Processing – NLP), como conversas, reconhecimento de fala, tradução e resumo, recorreremos a modelos de linguagem para nos ajudar.

Os modelos de linguagem podem aprender uma biblioteca de texto (chamada corpus) e prever palavras ou sequências de palavras com distribuições probabilísticas, ou seja, a probabilidade de ocorrência de uma palavra ou sequência. Por exemplo, quando você diz “**O Tom gosta de comer…**”, a probabilidade da próxima palavra ser “**pizza**” seria maior do que “**mesa**”. Se estiver prevendo a próxima palavra na sequência, é chamada de previsão do próximo token; se estiver prevendo uma palavra ausente na sequência, é chamado de modelagem de linguagem mascarada.

Como é uma distribuição de probabilidade, pode haver muitas palavras prováveis ​​com probabilidades diferentes. Embora você possa pensar que o ideal é escolher sempre o melhor candidato com a maior probabilidade, isso pode levar a sequências repetitivas. Assim, na prática, os pesquisadores adicionariam alguma aleatoriedade (temperatura) ao escolher a palavra entre os principais candidatos.

![[Pasted image 20241003140308.png]]

2017 – O Google publicou o artigo *Attention is All You Need*, que apresentou a arquitetura do transformer.

## Arquitetura do transformers

A arquitetura do _transformer_ é a base do GPT. É um tipo de rede neural, que é semelhante aos neurônios do nosso cérebro humano. O _transformer_ pode compreender contextos em dados sequenciais como texto, fala ou música melhor com mecanismos chamados de **atenção e autoatenção**.

![[Pasted image 20241003140535.png]]

Os transformers possuem os seguintes componentes:

- **Incorporação e Encoding Posicional:** transformando palavras em vetores de números
- **Encoder:** extrai recursos da sequência de entrada e analisa o significado e o contexto dela. Ele gera uma matriz de estados ocultos para cada token de entrada a ser passado para o decodificador
- **Decoder:** gere a sequência de saída com base na saída do codificador e nos tokens de saída anteriores
- **Camadas lineares e não lineares (Softmax):** transformando o vetor em uma distribuição de probabilidade de palavras de saída

O encoder e o decoder são os principais componentes da arquitetura do transformer. O encoder é responsável pela análise e “compreensão” do texto de entrada e o decoder é responsável pela geração de saída.

## Mas como é a arquitetura do GPT e outros LLMs?


O nome completo do GPT é **Generative Pre-trained Transformer** (algo como Transformador Generativo Pré-treinado). Pelo nome, você pode ver que é um modelo generativo, bom para gerar resultados; é pré-treinado, o que significa que aprendeu com um grande corpus de dados de texto; é um tipo de _transformer_.

Na verdade, o GPT usa apenas a parte do decoder da arquitetura do _transformer._ Na seção de _transformers_ anterior, aprendemos que o decoder é responsável ​​por prever o próximo token na sequência. **A GPT repete esse processo várias vezes usando os resultados gerados anteriormente como entrada para gerar textos mais longos**, os quais são chamados de **autorregressivos**.

#  O que são os LLM (grandes modelos de linguagem)?

Large Language Models (LLMs) como GPT-4o da OpenAI são algoritmos de aprendizado de máquina projetados para compreender e gerar texto semelhante ao humano com base nos dados nos quais foram treinados. Esses modelos são construídos usando redes neurais com milhões ou até bilhões de parâmetros, tornando-os capazes de realizar tarefas complexas como tradução, resumo, resposta a perguntas e até redação criativa.

Treinados com fonte de dados extensas como:
- Pesquisas cientificas,
- Partes da Internet,
- Enciclopédias,
- Livros
- etc.

Os LLMs analisam os padrões e relações entre palavras e frases para gerar resultados coerentes e contextualmente relevantes.

## O que é LangChain

- Desenvolvido por Harrison Chase.
- Lançado em 2022

**LangChain** é uma estrutura (framework) open source desenvolvida para facilitar o desenvolvimento de aplicativos e pipelines que integram modelos de linguagem de grande porte (LLMs) com outras fontes de dados e componentes de software. 

O objetivo principal do LangChain é ajudar desenvolvedores a criar aplicações complexas que podem usar modelos de linguagem de maneira eficiente e interagir com fontes de dados externas, como bancos de dados, APIs, e outros tipos de dados dinâmicos.

O LangChain permite ao desenvolvedor criar um ambiente centralizado para construir aplicações LLMs e integrá-los ao seu sistema. 

As abstrações são um elemento comum da vida cotidiana e da linguagem. Por exemplo, "_π_" nos permite representar a razão entre o comprimento da circunferência de um círculo e o de seu diâmetro sem ter que escrever seus dígitos infinitos. Da mesma forma, um termostato nos permite controlar a temperatura em nossa casa sem precisar entender os circuitos complexos que isso implica. Basta sabermos como diversas configurações de termostato se traduzem em temperaturas diferentes.

O LangChain é essencialmente uma biblioteca de abstrações para Python e Javascript, representando etapas e conceitos comuns necessários para trabalhar com modelos de linguagem. Esses componentes modulares, como funções e classes de objetos, servem como blocos de construção de programas de IA generativos. Podem ser "_encadeados_" para criar aplicativos, minimizando a quantidade de código e o entendimento refinado necessário para executar tarefas complexas de PNL. 

## Componentes Fundamentais:

- **Models (Modelos):** LangChain serve como uma interface padrão que permite interações com uma ampla gama de Grandes Modelos de Linguagem (LLMs).

- **Chains (Cadeias):** Como seu nome implica, _cadeias_ são o núcleo dos fluxos de trabalho do LangChain. Combinam LLMs com outros componentes, criando aplicativos por meio da execução de uma sequência de funções.

- **Prompts (Instruções):** Os prompts são as instruções apresentadas a um LLM. Geralmente, a "arte" de redigir prompts que efetivamente entregam o contexto necessário para que o LLM interprete a entrada e a saída da estrutura da maneira mais útil para você é chamada de engenharia de prompt.

- **Indexes (Índices):** Para realizar determinadas tarefas, as LLMs precisarão acessar fontes de dados externas específicas não incluídas em seu conjunto de dados de treinamento, como documentos internos, e-mails ou conjuntos de dados. LangChain refere-se coletivamente a essa documentação externa como “índices"."

- **Memory (Memória):** Por padrão, os LLMs não têm memória de longo prazo de conversas anteriores (a menos que o histórico do chat seja usado como entrada para uma consulta). O LangChain soluciona esse problema com utilitários simples para adicionar memória a um sistema, com opções que vão desde a retenção total de todas as conversas até a retenção de um resumo da conversa até a retenção das _n_ trocas mais recentes.

- **Agents/Tools (Agentes/Ferramentas):** Os agentes do LangChain podem usar um determinado modelo LLM como um "mecanismo de raciocínio" para determinar quais ações tomar. Ao criar uma cadeia para um agente, as entradas contêm:

	- uma lista de ferramentas disponíveis para serem aproveitadas.
	- entrada do usuário (como prompts e consultas).
	- quaisquer etapas relevantes executadas anteriormente.

## Como esses componentes se conectam?

Hoje o langchain está na versão [v0.3](https://python.langchain.com/docs/introduction/) e desde a versão anterior foi definido um protocolo de construção de chains: a linguagem LCEL (LangChain Expression Language): maneira declarativa de encadear componentes do LangChain. Benefícios:

- **Suporte de streaming de primeira classe**.
- **Suporte assíncrono**.
- **Execução paralela otimizada**.
- **Novas tentativas e fallbacks**.
- **Acessar resultados intermediários**.
- **Esquemas de entrada e saída**.
- **Rastreamento**.

Grande parte dos componentes do LangChain segue o protocolo `Runnable` (executáveis) que automaticamente implementa as interfaces de `invoke`, `stream` e `batch` bem como suas variantes assincronas.

O `Runnables` é a unidade de trabalhado do LangChain ou seja, uma vez que o componente é criado usando como base os '`Runnables`' , este componente adquire a capacidade de  ser invocado, agrupado, transmitido, transformado e composto. 

Vantagem de usar a linguagem LCEL:
- Código Limpo
- Fácil Manutenção
- Simplicidade

