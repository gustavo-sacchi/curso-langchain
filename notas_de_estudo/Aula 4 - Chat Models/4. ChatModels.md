
# O que são?

Modelos de chat são interfaces LangChain que permitem a comunicação com os provedores de modelos de linguagem natural. Os ChatModels usam uma sequência de mensagens como entradas e retornam mensagens como saídas. 

Diferentes dos modelos de texto simples (*Completion Style*) que agem apenas completando a entrada do usuário, os modelos do tipo chat (*Conversation Style*) trabalham no sentido a criar conversas, ou seja, com perguntas e respostas e, por isso, oferecem suporte à atribuição de funções distintas a mensagens de conversação, ajudando a distinguir mensagens da IA, usuários e instruções, como mensagens do sistema.

Veja na documentação à quais provedores o LangChain oferece suporte para a interface de comunicação ChatModels: https://python.langchain.com/docs/integrations/chat/
# Parâmetros

Temos alguns parâmetros padronizados ao construir ChatModels:

- `model`: o nome do modelo
- `temperature`: a temperatura de amostragem
- `timeout`: tempo limite da solicitação
- `max_tokens`: máximo de tokens para gerar
- `stop`: sequências de parada padrão
- `max_retries`: número máximo de vezes para repetir solicitações
- `api_key`: Chave de API para o provedor do modelo
- `base_url`: ponto final para enviar solicitações

# O que os ChatModels recebem como entrada?


Os ChatModels recebem como entrara um conjunto de mensagens (uma lista) e retornam uma mensagem do tipo 'assistente' (ai). Logo, todas as mensagens têm uma propriedade `role`, `content`, e .`response_metadata`

O `role`descreve QUEM está dizendo a mensagem. Os papéis padrão são "usuário", "assistente", "sistema" e "ferramenta". LangChain tem diferentes classes de mensagem para diferentes papéis.

A `content`propriedade descreve o conteúdo da mensagem. Isso pode ser algumas coisas diferentes:

- Uma sequência (a maioria dos modelos lida com esse tipo de conteúdo)
- Uma lista de dicionários (usada para entrada multimodal, onde o dicionário contém informações sobre esse tipo de entrada e esse local de entrada)

Tipos de menagens no LangChain:

- HumanMessage = representa entrada do usuário
- AIMessage = representa a resposta do modelo (função assistente). Neste caso pode vir com uma instrução de chamada de função (`tool_calls`).

# Exemplo 1:

Vamos começar com um exemplo mais simples de chamada à esses modelos. Lembrando, é necessário ter uma chave API no arquivo ".env". Se você não sabe o que estou falando, assista ao vídeo da Aula 1 - [[1. Configurando o Ambiente Virtual|Configurando o Ambiente Virtual]].

Vamos fazer uma chamada ao modelo Chat GPT 40:

```python

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

load_dotenv()

model = ChatOpenAI(model = "gpt-4o", temperature = 0.1)

# O CHatModel é um componente LangChain então ele possui o protocolo invoke()

resposta = model.invoke("Olá como você está e o que você é capaz de fazer?")

print("--------RESPOSTA AIMessage:---------")
print(resposta)
print("-------------------------------------")

print("--------RESPOSTA Somente Texto:------")
print(resposta.content)
print("-------------------------------------")
```

Nesse caso como a entrada do usuário foi passado diretamente pelo `invoke`, automaticamente o LangChain entende que é uma entrada do tipo `HumanMessage`.

Você observa que retorna a resposta dentro de um componente "AIMessage" de LangChain. Para acessar é necessário, como falamos, imprimir o `.content`.

# Exemplo 2

Vamos agora criar uma conversação usando os componentes de "Message" para interagir com os modelos de linguagem.

Partiremos do exemplo anterior e agora vamos criar uma lista de mensagens para que o modelo entenda o que está sendo conversado. Vamos atribuir um prompt de sistema e um conjunto de mensagens para simular uma conversa que já começou com algumas trocas de informações:


```python

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

load_dotenv()

model = ChatOpenAI(model = "gpt-4o", temperature = 0.1)

## Criando a conversa. Lembrando que os ChatModels recebem como entrada uma lista de mensagem. Assim o LangChain automaticamente converte isso na estrutura que o modelo LLM precisa receber para responder.

# Forma 1 de escrever:
mensagens = [
			 SystemMessage(content="Você é um especialista em astrofísica."),
			 HumanMessage(content="Qual a distancia do sol até a terra?"),
			 AIMessage(content="O Sol está a 49.600.000 km de distância da Terra."),
			 HumanMessage(content="E a distância da terra até marte?"),
]

# Forma 2 de escrever:
# mensagens = [
# 			 ("system", "Você é um especialista em astrofísica."),
# 			 ("user", "Qual a distancia do sol até a terra?"),
# 			 ("assistant", "O Sol está a 49.600.000 km de distância da Terra."),
#            ("user", "E a distância da terra até marte?"),
# ]

# Como a entrada do usuário é a ultima mensagem da lista, você pode dá invoke usando a lista de pensagens contendo o histórico de conversação.
resposta = model.invoke(mensagens)

print("--------RESPOSTA AIMessage:---------")
print(resposta)
print("-------------------------------------")

print("--------RESPOSTA Somente Texto:------")
print(resposta.content)
print("-------------------------------------")
```

Certo, mas e se eu quiser utilizar outro provedor? No LangChain com uma simples mudança, você consegue já a se comunicar com outro modelo sem alterações bruscas no seu código. Para os exemplos de cima, basta:

Cadastrar no arquivo `.env` a chave API. Depois importar o ChatModel do provedor de preferência. O resto fica tudo igual.

Exemplo usando o Claude da Anthropic: Cadastrar: `ANTHROPIC_API_KEY` e ajustar as linhas do `model` para:

```python

from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model = "claude-3-5-sonnet-20240620", temperature = 0.1)

# [...] O resto continua igual.
```

Obs: Talvez será necessário importar as dependências: `pip install langchain-anthropic`

# Exemplo 3

Vamos agora criar um chat, ou seja, vamos criar uma lista que vai crescendo dinamicamente com a entrada do usuário simulando uma conversa com ChatGPT.

```python
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage


load_dotenv()
model = ChatOpenAI(model = "gpt-4o", temperature = 0.1)

conversa = [SystemMessage(content="Você é um assistente útil que responde ao usuário com detalhes e exemplos.")]


while True:
    entrada = input("Entrada Usuário (digite 'q' para parar.): ")
    if entrada.lower() == "q":
        break
    
    conversa.append(HumanMessage(content=entrada))
    
    resultado = model.invoke(conversa)
    resposta = resultado.content
    conversa.append(AIMessage(content=resposta))
    
    print(f"Resposta IA: {resposta}")


print("---- Histórico Completo ----")
print(conversa)
```

# Exemplo 4

Vamos agora simular o streaming de dados dos modelos (quando compatível) onde cada *token* é gerado em tempo de execução. Vamos repetir o código anterior e ao invés de `invoke`, vamos chamar o modelo usando a função assíncrona `astream`:

```python
from dotenv import load_dotenv  
from langchain_openai import ChatOpenAI  
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage  
  
load_dotenv()  
  
async def conversa_com_modelo():  
    model = ChatOpenAI(model="gpt-4o", temperature=0.1)  
    conversa = [SystemMessage(content="Você é um assistente útil que responde ao usuário com detalhes e exemplos.")]  
  
    while True:  
        entrada = input("\nEntrada Usuário (digite 'q' para parar.): ")  
        if entrada.lower() == "q":  
            break  
  
        conversa.append(HumanMessage(content=entrada))  
  
        # Ajustado aqui:  
        all_chunk = []  
        async for chunk in model.astream(conversa):  
            all_chunk.append(chunk.content)  
            print(chunk.content, end="", flush=True)  
  
        # Para armazenar na conversa a resposta do modelo ou caso você queira ver a resposta final você precisará  
        # juntar os chunks armazenados em `all_chunk` antes:        resposta_completa = "".join(all_chunk)  
        conversa.append(AIMessage(content=resposta_completa))  
        # print(f"\nResposta final: {resposta_completa}")  
  
    print("---- Histórico Completo ----")  
    print(conversa)  
  
import asyncio  # Biblioteca para lidar com funções assíncronas  
asyncio.run(conversa_com_modelo())
```


Sobre os ChatModels estas são as principais informações que você precisa conhecer para  começar a implementar a comunicação com as APIs de modelos LLMs.  