## 1. O que é Chain

Como comentamos nas aulas anteriores, chain é um pipeline de componentes do langchain para que possamos realizar a iteração entre nosso sistema e os modelos LLMs.

Para ficar mais fácil o entendimento, uma chain é minimamente construída encapsulando um Prompt Template (assunto da aula de hoje) e a chamada do modelo de linguagem (chat models apresentado na aula anterior).

## 2. Prompt Template

**Definição**: Os modelos de prompt (prompt template) ajudam a traduzir a entrada e os parâmetros do usuário em instruções para um modelo de linguagem. Ou seja, a partir da entrada do usuário, o langchain vai traduzir sua entrada em um "prompt" no formato de `string` antes de enviar ao modelo para que seja feita a geração de resposta.

**Formato**: Os modelos de prompt recebem como entrada um dicionário, onde cada chave representa uma variável no modelo de prompt a ser preenchida.

**Ele é um componente LangChain?** Sim, ele é implementado usando a classe base `runnable` ou seja, implementa os métodos de `invoke`.

Documentação: [link](https://python.langchain.com/docs/how_to/#prompt-templates)

## 2.1 Tipos:

### String PromptTemplates

Os `PromptTemplates` do tipo `strings`, são usados para formatar uma entrada mais simples. Além disso, você pode substituir variáveis em tempo de execução, ou seja, usando chaves "{}" você consegue subistituir as variaveis do prompts para um valor correspondente uma vez que um `PromptTemplate`recebe um dicionário (`{"topico": "papagaios"}`) como entrada.

Em resumo:
- `PromptTemplate` são os estados de prompts mais simples que formatam uma string.
- Recebem como entrada um dicionário.
- Dado que eles recebem um dicionário, ocorre um mapeamento das variáveis que serão substituídas pelo correspondente valor dado pelas tags `{}`.


```python
from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template("Me conte uma história sobre {topico}")

prompt_template.invoke({"topico": "papagaios"})
```

No nosso exemplo, `topico` presente no prompt é substituído por `gatos`. Assim, o LLM enxergará: `Me conte uma história sobre papagaios`

### ChatPromptTemplates

Os `ChatPromptTemplates` formatam uma lista de mensagens. [[5. Prompts Templates|Na aula anterior ]]verificamos que os Chat Models recebem uma lista de mensagens (que podem ser do tipo: ia, sistema ou usuário) formatando uma estrutura de `chat` para o LLM. Os `ChatPromptTemplates` estruturam as entradas em uma lista de mensagens. Por isso, eles são gerados da seguinte forma:

```python
from langchain_core.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate([
    ("system", "Você é um assistente de ia util"),
    ("user", "Conte-me uma história sobre {topico}")
])

prompt_template.invoke({"topico": "papagaios"})
```
Observe que o `conteúdo` deles seguem as mesmas regras do `PromptTemplates` simples, ou seja, recebem como entrada também um dicionário e as variáveis do prompt são substituídas antes de ser enviadas ao LLM.

Vale ressaltar que for enviado apenas um dicionário para para `ChatPromptTemplate`, automaticamente essa mensagem será uma mensagem do tipo humana:

```python
from langchain_core.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate("Conte uma história sobre {topico}")

prompt_template.invoke({"topico": "papagaios"})

# A saida disso será uma lista contendo uma mensagem humana: [HumanMessage(content="Conte uma história sobre papagaios")]
```

### MessagesPlaceholder

Os `MessagesPlaceholder` também são um dos tipos de prompt template mas estes adicionam uma mensagem (`message`) em uma determinada posição na lista de mensagens. Por exemplo, digamos que eu desejo enviar uma mensagem do tipo usuário mas eu já tenho uma lista que inclui no meu prompt uma mensagem de sistema, eu poderia usar `MessagesPlaceholder` para colocar no prompt essa `user message`, ou seja:

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage

prompt_template = ChatPromptTemplate([
    ("system", "Você é um assistente de ia util"),
    MessagesPlaceholder("msgs")
])

prompt_template.invoke({"msgs": [HumanMessage(content="Olá!")]})

# O resultado da lista seria:
# [("system", "Você é um assistente de ia util"),
# ("user", "Olá!")]
```


## 3. Prática

Aqui vamos explorar algumas formas de trabalhar com os modelos de prompt

#### Exemplo 1 - Simples (formato de saída `string`)

```python
from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template("Gere para mim um poema sobre: {assunto}. Escreva em {lingua}")

retorno = prompt_template.invoke({"assunto": "navegação", "lingua":"pt-br"})

print(retorno)

```

Ao fazer o `invoke` você verá que a saida é uma string.

#### Exemplo 2 - Modelos de Chat (formato de saída lista de `message`)

```python
from langchain_core.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate(["Gere para mim um poema sobre: {assunto}. Escreva em {lingua}"])

retorno =prompt_template.invoke({"assunto": "navegação", "lingua":"pt-br"})
print(retorno)

```

Ao fazer o `invoke` você verá que a saída é uma lista contendo a mensagem do tipo humana (`HumanMessage`). Você também pode implementar da seguinte forma:

```python
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_core.messages import HumanMessage

prompt_template = ChatPromptTemplate([HumanMessagePromptTemplate.from_template("Gere para mim um poema sobre: {assunto}. Escreva em {lingua}")])

retorno = prompt_template.invoke({"assunto": "navegação", "lingua":"pt-br"})
print(retorno)
```

```python
from langchain_core.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate([("user", "Gere para mim um poema sobre: {assunto}. Escreva em {lingua}")])

retorno = prompt_template.invoke({"assunto": "navegação", "lingua":"pt-br"})
print(retorno)
```

todas elas funcionam da mesma forma, seja utilizando a forma explicita ou subentendida.

#### Exemplo 3 - Modelos de Chat (lista de mensagens incluindo sistema):

```python
from langchain_core.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate([
									  ("system", "Você é um assistente de IA com habilidade de escritor de poesia."),
									  ("user", "Gere para mim um poema sobre: {assunto}. Escreva em {lingua}")])

retorno =prompt_template.invoke({"assunto": "navegação", "lingua":"pt-br"})
print(retorno)
```

Lembre-se que também existe outras formas similares de implementação:

- Forma 1

```python
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate


prompt_template = ChatPromptTemplate([HumanMessagePromptTemplate.from_template("Gere para mim um poema sobre: {assunto}. Escreva em {lingua}")])  
  
retorno = prompt_template.invoke({"assunto": "navegação", "lingua":"pt-br"})  
print(retorno)
```

- Forma 2 - Usando `MessagesPlaceholder` para substituir o elemento da lista de mensagens pela mensagem do usuário.

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt_template = ChatPromptTemplate([
									  ("system", "Você é um assistente de IA com habilidade de escritor de poesia."),
									  MessagesPlaceholder("msgs_user")])

retorno =prompt_template.invoke({"msgs_user": [HumanMessage(content="Gere para mim um poema sobre: navegação. Escreva em pt-br")]})
print(retorno)
```

#### Exemplo 4 - Usando ChatModels + ChatPromptTemplate

Até o momento apenas invocamos os prompts, sem que este tivessem conectado à um modelo, vamos agora interagir com modelos LLM usando os nossos prompts.

Podemos utilizar os conhecimentos da aula anterior onde aprendemos sobre os Chat Models e criar nossas primeiras chains interagindo com o modelo usando os modelos de prompts.

```python
from dotenv import load_dotenv
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# Carregamento das variáveis de ambiente presentes em .env
load_dotenv()

# Criando o componente de langchain que iterage com os LLMs
model = ChatOpenAI(model="gpt-4o")

### Exemplo 1
# PART 1: Criando ChatPromptTemplate
print("-----Exemplo Chain 1 -----")

prompt_template = ChatPromptTemplate([("user", "Escreva um poema em {lingua} sobre o tema: {assunto}")])

# PART 2: Criando a chain
chain1 = prompt_template | model

# PART 3: Invoke da chain passando as variáveis.
resposta = chain1.invoke({"lingua": "pt-br", "assunto":"frutas"})

print(resposta.content)


### Exemplo 2
# PART 1: Criando ChatPromptTemplate já com mensagem de sistema:
print("-----Exemplo Chain 2 -----")

mensagens = [
    ("system", "Você é um poeta brasileiro famoso e escreve poemas de no máximo {n_versos} versos."),
    ("human", "Escreva para mim um poema sobre {assunto}."),
]

prompt_template = ChatPromptTemplate(mensagens)

# PART 2: Criando a chain
chain2 = prompt_template | model

# PART 3: Invoke da chain passando as variáveis.
resposta = chain2.invoke({"n_versos": "10", "assunto":"navios"})

print(resposta.content)

```