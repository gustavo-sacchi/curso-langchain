
# Definição:

[Documentação](https://python.langchain.com/docs/concepts/output_parsers/)

Os analisadores de saída são responsáveis ​​por pegar a saída de um LLM e transformá-la em um formato mais adequado. Isso é muito útil quando você está usando LLMs para gerar qualquer forma de dados estruturados.

Como sabemos os modelos LLMs podem gerar respostas diferentes para uma mesma pergunta, ou seja, eles não são determinísticos. Por isso, os OutputParsers são componentes do langchain que formatam a saída do modelos em uma estrutura predefinida, seja uma simples string da resposta ou um formato JSON, por exemplo.

Além de ter uma grande coleção de diferentes tipos de analisadores de saída, um benefício diferenciado do LangChain OutputParsers é que muitos deles oferecem suporte a streaming (ou seja, funcionam com a transmissão de dados token a token).

Outro ponto fundamental é que os OutputParsers implementam interface `Runnables`, ou seja,  são componentes de LangChain e implementam as funções `invoke`, `ainvoke` etc. Sua entrada pode ser uma string, ou um `BaseMessage` (mensagem obtida ao invocar um modelo usando `ChatPromptTemplate`).

# Contextualização

Até o momento nós capturávamos as respostas do modelo dentro de uma chain acessando o conteúdo da `AIMessage` resposta. Porém há uma forma mais 'bonita' de se obter a resposta, utilizando um analisador de saída do tipo string:

Exemplo, considere uma simples chain:
```python
from dotenv import load_dotenv
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# Carregamento das variáveis de ambiente presentes em .env
load_dotenv()

# Criando o componente de langchain que iterage com os LLMs
model = ChatOpenAI(model="gpt-4o")

### Exemplo 1

prompt_template = ChatPromptTemplate([("user", "Escreva um poema em {lingua} sobre o tema: {assunto}")])

# PART 2: Criando a chain
chain1 = prompt_template | model

# PART 3: Invoke da chain passando as variáveis.
resposta = chain1.invoke({"lingua": "pt-br", "assunto":"frutas"})

print(resposta.content)

```

Nesse caso a resposta será um componente `AIMessage` e estamos acessando o conteúdo da resposta por meio de `.content`. Porém você pode fazer a seguinte combinação:

```python
[...]

from langchain_core.output_parsers import StrOutputParser

analisador_saida = StrOutputParser()

chain1 = prompt_template | model | analisador_saida

resposta = chain1.invoke(...)

[...]
```

Ao executar, você obterá em 'resposta' o conteúdo gerado pelo LLM sem necessitar realizar o acesso pela variável `content`. 

Agora imagine que você deseja formatar uma resposta de um LLM em um JSON para utilizar o resultado como parâmetro de uma função.  Não podemos simplesmente utilizar o `StrOutputParser` e torcer que o prompt está 100% adequado para que o LLM gere uma resposta sempre igual e estruturada em JSON. Além disso, ao utilizar este analisador de saída, teremos uma string e não um componente JSON. Para resolver esse problema o LangChain possui alguns analisadores de saídas prontos.

Por exemplo, digamos que eu tenho a seguinte função que recebe um dicionário de entrada esperando que exista o parâmetro 'escolha' como chave e um número como valor:

```python
def rota(resultado: dict):
	if resultado["escolha"] == 1:
		print("A escolha foi opção 1.")
	elif resultado["escolha"] == 2:
		print("A escolha foi opção 2.")
	else:
		print(f"A escolha foi uma opção que não foi nem 1 e nem 2, foi: {resultado["escolha"]}.")
```

O LLM por mais que você estruture o prompt pode soltar respostas, diferentes a cada execução, por exemplo:

```
saida = """
A resposta é: {"escolha":"1"}
"""
```

ou

```
{"escolha":1}
```

ou

```
Com base na sua solicitação:

```json
{"escolha":1}
´´´
Espero que tenha ajudado!
```

Veja que existe várias formas que o LLM pode responder. Assim para resolver essa situação, você pode impor ao LLM a estruturação de saída, forçando ele a responder com base numa estrutura que entregamos para ele. Adiante, citamos as principais e mais utilizadas estruturas. Existem outras e você pode encontrar elas na [documentação oficial](https://python.langchain.com/docs/concepts/output_parsers/).

# Output Parsers

### StrOutputParser
[Documentação](https://python.langchain.com/docs/how_to/output_parser_string/)

StrOutputParser simplifica a extração de texto de objetos de mensagem. Exemplo:

```python
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# Carregamento das variáveis de ambiente presentes em .env
load_dotenv()

model = ChatOpenAI(model="gpt-4o")

chain = model | StrOutputParser()

response = llm.invoke("Olá")

# Type response => str
print(response) # resposta: Olá tudo bem? Em que posso ajudar?
```

### PydanticOutputParser
[Documentação](https://python.langchain.com/docs/how_to/output_parser_structured/)

Os modelos de linguagem produzem texto. Mas há momentos em que você quer obter informações mais estruturadas do que apenas texto de volta. Embora alguns provedores de modelos suportem maneiras integradas de retornar saída estruturada , nem todos o fazem.

Os analisadores de saída são classes que ajudam a estruturar respostas de modelos de linguagem.

No caso do tipo PydanticOutputParser, é implementada uma saída estruturada usando a classe Pydantic, onde você declara quais variáveis deve ter na saída do LLM para criar uma instancia da sua classe pydantic. Exemplo:

```python
from dotenv import load_dotenv  
load_dotenv()  
  
from langchain_core.output_parsers import PydanticOutputParser  
from langchain.prompts import ChatPromptTemplate  
from langchain_openai import ChatOpenAI  
from pydantic import BaseModel, Field  
  
model = ChatOpenAI(model="gpt-4o")  
  
# Definindo a minha estrutura de saída  
class Rota(BaseModel):  
    escolha: int = Field(description="Rota escolhida")  
    pensamento: str = Field(description="Campo para o pensamento que levou a decisão da rota escolhida")  
  
  
# Criando o analisador de saída  
parser = PydanticOutputParser(pydantic_object=Rota)  
  
prompt_template = ChatPromptTemplate([("system",  
                                       "Se a pergunta do usuário for relacionado ao setor financeiro, a escolha deve ser 1, caso contrário a escolha pode ser qualquer numero diferente de 1. \n{format_instructions}\n Pergunta Usuário: {pergunta_user}")],  
                                     partial_variables={"format_instructions": parser.get_format_instructions()})  
  
  

prompt_and_model = prompt_template | model  
output = prompt_and_model.invoke({"pergunta_user": "Me diga quanto está o dollar."})  
print(output)
```

Ao executar esse teste, a resposta do LLM será:

```
content='```json\n{\n  "escolha": 1,\n  "pensamento": "A pergunta do usuário é relacionada ao setor financeiro, pois ele está perguntando sobre a cotação do dólar."\n}\n```' 
```

Veja que ele formatou a resposta entre a notação `json`. Dessa forma, é possivel agora encadear na chain o analisador de saida para que tenhamos como resposta uma instancia de `Rota`, ou seja, realizando a adaptação na chain encadeando o `parser`:

```python
prompt_and_model = prompt_template | model | parser
```

Teremos em `output`:

```python
output = Rota(escolha=1, pensamento='A pergunta do usuário é relacionada ao setor financeiro, pois ele está perguntando sobre a cotação do dólar.')
```

e podemos acessar os valores usando `output.escolha` e `output.pensamento`.

### JsonOutputParser
[Documentação](https://python.langchain.com/docs/how_to/output_parser_json/)

O `JsonOutputParser` é uma opção interna para solicitar e então analisar a saída JSON de um modelo LLM. 

Pode ser implementado usando Pydantic:

```python
from dotenv import load_dotenv  
load_dotenv()  
  
from langchain_core.output_parsers import JsonOutputParser  
from langchain.prompts import ChatPromptTemplate  
from langchain_openai import ChatOpenAI  
from pydantic import BaseModel, Field, model_validator  
  
model = ChatOpenAI(model="gpt-4o")  
  
# Definindo a minha estrutura de saída  
class Rota(BaseModel):  
    escolha: int = Field(description="Rota escolhida")  
    pensamento: str = Field(description="Campo para o pensamento que levou a decisão da rota escolhida")  
  
  
# Criando o analisador de saída  
parser = JsonOutputParser(pydantic_object=Rota)  
  
prompt_template = ChatPromptTemplate([("system",  
                                       "Se a pergunta do usuário for relacionado ao setor financeiro, a escolha deve ser 1, caso contrário a escolha pode ser qualquer numero diferente de 1. \n{format_instructions}\n Pergunta Usuário: {pergunta_user}")],  
                                     partial_variables={"format_instructions": parser.get_format_instructions()})  
  
  

prompt_and_model = prompt_template | model  | parser

output = prompt_and_model.invoke({"pergunta_user": "Me diga quanto está o dollar."})  
print(output)
```

Ou não usando Pydantic:

*Atenção: Você também pode usar o `JsonOutputParser`sem Pydantic. Isso fará com que o modelo retorne JSON, mas não fornece detalhes sobre qual deve ser o esquema. Ou seja, pode ser que o modelo erre a estrutura do JSON esperada.*

```python
from dotenv import load_dotenv  
load_dotenv()  
  
from langchain_core.output_parsers import JsonOutputParser  
from langchain.prompts import ChatPromptTemplate  
from langchain_openai import ChatOpenAI  
from pydantic import BaseModel, Field, model_validator  
  
model = ChatOpenAI(model="gpt-4o")  
  
# Criando o analisador de saída  
parser = JsonOutputParser()
  
prompt_template = ChatPromptTemplate([("system",  
                                       "Se a pergunta do usuário for relacionado ao setor financeiro, a escolha deve ser 1, caso contrário a escolha pode ser qualquer numero diferente de 1. \n{format_instructions}\n Pergunta Usuário: {pergunta_user}")],  
                                     partial_variables={"format_instructions": parser.get_format_instructions()})  
  
  

prompt_and_model = prompt_template | model  | parser

output = prompt_and_model.invoke({"pergunta_user": "Me diga quanto está o dollar."})  
print(output)
```

### CommaSeparatedListOutputParser
[Documentação](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.list.ListOutputParser.html)

Serve para criar a analise de saída de uma chamada LLM para uma lista. Ou seja, solicita ao modelo devolver uma resposta em uma lista separada por vírgula (\[`foo, bar, baz`\]). Nesse caso a saída será formatada em um objeto `List` do `python`.

```python
output_parser = CommaSeparatedListOutputParser()  

format_instructions = output_parser.get_format_instructions()  

prompt = PromptTemplate(  
template= "Liste cinco {assunto}.\n{format_instructions}" ,  
input_variables=[ "assunto" ],  
partial_variables={ "format_instructions" : format_instructions}  
)

model = ChatOpenAI(model="gpt-4o")  
cadeia = prompt | model | output_parser  
  
saída = chain.invoke({ "assunto" : "frutas" })
```
### Outros:

- XMLOutputParser: [Documentação](https://python.langchain.com/docs/how_to/output_parser_xml/)
- YamlOutputParser: [Documentação](https://python.langchain.com/docs/how_to/output_parser_yaml/)
- DatetimeOutputParser:[Documentação](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.datetime.DatetimeOutputParser.html#langchain.output_parsers.datetime.DatetimeOutputParser)
- EnumOutputParser: [Documentação](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.enum.EnumOutputParser.html#langchain.output_parsers.enum.EnumOutputParser)
- PandasDataFrameOutputParser: [Documentação](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser.html#langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser)

# E se der um erro?
[Documentação](https://python.langchain.com/docs/how_to/output_parser_retry/)

Como sabemos um LLM mesmo impondo para ele uma saída estruturada, ele pode errar, principalmente modelos menores e treinados com pouco conteúdo estruturado. Para sanar esse problema você pode pedir ao modelo para ele tentar novamente, caso tenha ocorrido algum erro.

Imagine o seguinte exemplo com uma resposta incompleta por parte do modelo.

```python
template = """Com base na pergunta do usuário, forneça uma Ação e uma Entrada de Ação para qual etapa deve ser tomada.
{format_instructions}
Pergunta: {query}
Resposta:"""


class Action(BaseModel):
    action: str = Field(description="ação a tomar")
    action_input: str = Field(description="entrada para a ação")


parser = PydanticOutputParser(pydantic_object=Action)

prompt = ChatPromptTemplate([("system", template)], partial_variables={"format_instructions": parser.get_format_instructions()})  
                                     

solicitacao_prompt = prompt.format_prompt(query="Quem é namorada de Leonardo DiCaprio?")

# Imagine que o modelo responder com uma saida faltando uma variável 'action_input'. Quando aplicado o parser no prompt, ocorrerá um erro de chave ausente.
resposta_incompleta = '{"action": "search"}'

try:
    parser.parse(resposta_incompleta)
except OutputParserException as e:
    print(e)
```

Para solucionar isso, podemos usar o `RetryOutputParser`, que passa o prompt (assim como a saída original) para o modelo LLM tentar novamente obter uma resposta melhor. Então:

```python
from langchain.output_parsers import RetryOutputParser

retry_parser = RetryOutputParser.from_llm(parser=parser, llm=OpenAI(temperature=0))

retry_parser.parse_with_prompt(resposta_incompleta, solicitacao_prompt)

# Nesse caso o LLM pode entender o problema e ajustar a resposta com ambas as chaves esperadas:
# Action(action='pesquisar', action_input='namorada do Leonardo DiCaprio')
```

Também podemos adicionar o RetryOutputParser facilmente com uma cadeia personalizada que transforma a saída bruta do LLM/ChatModel em um formato mais funcional.

```python
from langchain_core.runnables import RunnableLambda, RunnableParallel

completion_chain = prompt | OpenAI(temperature=0)

main_chain = RunnableParallel(
    completion=completion_chain, solicitacao_prompt=prompt
) | RunnableLambda(lambda x: retry_parser.parse_with_prompt(**x))


main_chain.invoke({"query": "Quem é namorada de Leonardo DiCaprio?"})
```